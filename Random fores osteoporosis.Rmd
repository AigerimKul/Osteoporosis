---
title: "Random Forest: Osteoporosis"
author: "Aigerim Kulzhabayeva"
date: "24 March 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## 1. Background.


This Markdown will introduce decision trees and Random forest models and Boosting method through an example: Prediction of osteoporosis fracture.

Osteoporosis is a condition that causes bones to become thin and porous, decreasing bone strength and leading to increased risk of bone fracture. The most common sites of osteoporotic fracture are the wrist, spine and hip. No single cause for osteoporosis has been identified.

Summaryof the analysis

1. Features: Unbalanced outcome variable, missing values (30% of the data set)


```{r echo=FALSE}

#Loading the packages and dd

#install.packages("sas7bdat")
#install.packages("tidyverse")
#install.packages("VIM")
#install.packages("infotheo")
#install.packages("mice")
#install.packages("randomForest")

library(sas7bdat)
library(infotheo)
library(VIM)
library(lattice)
library(latticeExtra)
library(tidyverse)
library(infotheo)
library(mice)
library(randomForest)
library(spida2)

data<-read.sas7bdat("C:/Users/ivanopolo/Desktop/Masters courses/frax_risk (2).sas7bdat")

```


## 2. Exploratory analysis.

First I would like to give variables new names that are more meningful to me. 

```{r}
#rename the variables 

data<-plyr::rename(data,replace=c("SEQN" = "id","RIAGENDR"= "gender", "RIDAGEYR"="age", "RIDRETH1"="race", "SDMVPSU"="masked.var",  "SDMVSTRA"="masked.strat","OSQ010A"= "broken.hip",  "OSQ010B"= "broken.wrist","OSQ010C"="broken.spine", "OSQ020A" = "num.broken.hip",  "OSQ020B" = "num.broken.wrist","OSQ020C" = "num.broken.spine",
  "OSQ040AA" = "age.first.hip", "OSQ040BA"="age.first.wrist", "OSQ040CA"="age.first.spine", "OSQ070"="test.ostep",  "OSQ130" = "steroid",  "OSQ170"= "mother.hip", "OSQ200"="father.hip",  "DXXOFBMD" = "femur.tot","DXXNKBMD"= "femur.neck", "DXXTRBMD"= "trochanter", "DXXINBMD"="intertrochanter", "DXXWDBMD"= "wards", "DXXOSBMD" = "spine.BMD", "ALQ101"= "12alc", "ALQ130"= "av.alc","ALQ140Q"= "5drinks.days", "DIQ010" ="diab","DID040"="age.diab",   "DIQ220" = "time.diab", "MCQ160A" = "arth",  "MCQ180A"= "age.arth","MCQ190"= "type.arth",   "MCQ160C"= "heart",  "MCQ180C"="age.heart", "MCQ160L"="liver", "MCQ170L" = "still.liver", "MCQ180L"= "age.liver",  "BMXBMI"= "BMI", "DBQ197" = "30day.milk","DBQ229"= "reg.milk.use",    "WHD020"= "weight.self",   "WHD110"= "weight10.self" ))

dim(data)

```


We have almost 600 obseravations on 54 variables. There are variables masked varance and stratified variance which account for the longitudinal nature of the data. Meaning that surveys were done in batches at different times. I will need to look further into how to incorporate these variables into the analysis.

### 1.1 Overview of the data

Lets first take a look at our data set graphically. We will pay attention to the classes of the variables, missing values, outliers as well as the shape of the quantile plots to get the insight about the distribution of the data.

```{r}
xqplot(data)  #Spida2 package
```

From the xqplot we see several things:

1. All variables are of class numeric, this will need to be change to the appropriate class.


2. Because numeric variables have missing values as 9999 its nearly imposible to see into the distribution of actual values.

3. There is an id variable that is unique for all samples and provides no information. We will remove that variable.

### 2.2 Assigning correct classes to the variables: numeric, factor, ordered factor. 

Lets now look at xqplot with updated classes of variables.

```{r}

#we will explore using data and work on dd

dd<-data
dd$id<-NULL
dd[dd=="NaN"]<-NA

#Factors
dd[ ,c("gender", "race","masked.var","broken.hip","broken.wrist","broken.spine","age.first.hip","age.first.wrist","age.first.spine","test.ostep","steroid","OSQ140U","mother.hip","father.hip","SMQ020","SMQ040","12alc","diab","arth","type.arth","heart","liver",
       "still.liver")]<-lapply(dd[ ,c("gender", "race","masked.var","broken.hip","broken.wrist","broken.spine","age.first.hip","age.first.wrist","age.first.spine","test.ostep","steroid","OSQ140U","mother.hip","father.hip","SMQ020","SMQ040","12alc","diab","arth","type.arth","heart","liver","still.liver")], as.factor)


#Ordinal variables

dd[,c("num.broken.hip","num.broken.wrist","num.broken.spine","time.diab","12alc","30day.milk","reg.milk.use")]<-lapply(dd[,c("num.broken.hip","num.broken.wrist","num.broken.spine","time.diab","12alc","30day.milk","reg.milk.use")],as.ordered)


xqplot(dd)

```

1. Variable race is unbalanced
2. WTMEC2YR? distibution is scewed
3. Varibles broken are very unbalanced
4. There are a lot of variables with a missing data, some over 90% missing.
5. For variables OSQ140Q, alc, 5drinkdays, age.diab, age.heart etc. are imposible to see the distribution because missing values are give value 9999.
6. Variable BMI is highly skewed


### 2.3 The outcome variable.

After looking at the data we realize that there seems to be no explicitly stated outcome variable. Lets at this stage formulate the questions we would like to explore and the outcome variable we will be working with.

Three variables seem to be good candidates and these are:

1. Binary variable -whether hip was broken
2. Binary variable -whether wrist was broken
3. Binary variable -whether spine was broken

#### The question we will be answering in this analysis is:

1. Can we predict a fracture
2. Which variables are associated with risk of fracture.

We will create new variable called "fracture" and will merge all types of fracture and create a binary variable woth two values: fracture occured (yes), fracture hasn't occured (no).


Later it would be interesting to also see if we can build a more refined  model by specifying particular location of the fracture as well as number of fractures using variables:

1. Ordinal - number of times hip was broken
2. Ordinal - number of times wrist was broken
3. Ordinal - number of times spine was broken

So for now we can remove all the outcome variables. 

Possible questions: does number of fractures you already had increase your risk for having more fractures? Probabily yes, but I conjecture that it is because previous fracture is a mediating variable.


```{r}

# Creating the outcome variable

dd$fracture<-0
dd$fracture[dd$broken.hip==1]<-1
dd$fracture[dd$broken.wrist==1]<-1
dd$fracture[dd$broken.spine==1]<-1

dd<-select(dd,-c("broken.hip","broken.wrist","broken.spine","num.broken.hip","num.broken.wrist","num.broken.spine")) #Removing other possible outcome varaibles

c<-table(dd$fracture) %>% as.data.frame()

barplot(table(dd$fracture)/5935, xlab= "Fracture", ylab = "Proportion",  horiz=FALSE, width = 0.2, xlim = (0:1), col = "#69b3a2", ylim = (0:1), names.arg = c("Yes", "No"), main = ("Proportion of Fractures in the Sample"))


```


We see that the outcome variable is quite unbalanced. Now that we have identified the outcome variable and formulated the question lets move on to data cleaning. We will start with adressing the missing values. 

### 2.4 Dealing with Missing Values

From the xqplot we see that there are quite a few missing values. Lets look at the variables that have missing values. We will try to understand the reason behind missingness and try to remedy it by looking at individual variables. We create a data set ddm which will include all the variables that have missing values.

```{r}
missing<-sum(is.na(dd))/(nrow(dd)*ncol(dd))
missing

```

From the whole data set we have about 30% of data missing.

```{r}
ddm<-dd[which(colSums(is.na(dd)) >0)]
dim(ddm)
aggr(ddm, oma = c(8,2,0,0)) 

```


There are 31 variables that have missing values, which is more than half of the variables. 

### 2.4.1 Keeping track of original and imputed variables.

At this stage I will start keeping track of variables that we have filled in the missing values for vs. the ones that had original information so when we will come to modelling we can add them to the model acordingly. We will put more faith in the data provided originally and less faith in the data imputed. data set dd.full will contain variables that originally had no missing values and we will be adding variables where we can fill in missing values confidently because they were missing by design. For example: for the variable of number of cigarettes smoked per day we can write zero if person doesn't smoke insted of NA - "Not applicable".

```{r}

dd.full<-select(dd,-c(names(ddm)))
dd.full
dim(dd.full)
          
```

### 2.4.2 Variable that have more than 90% missing values.

Because we have quite a few variables that have missing data lets look at them in stages by focusing first on the ones that have more than 90% missing values.

```{r}

ddm.90<-dd[which(colSums(is.na(dd)) >5935*0.9)] 
dim(ddm.90)
aggr(ddm.90, oma = c(8,1,0,0)) 

```

So we went down to 10 variables, which is a bit more managable. Lets look at the plot of missing values again.

Lets look at the variables that (age.first.hip, age.first.wrist, age.first.diab). I noticed that these variables are factors with value of 1 when fracture occured after age of 50 and have NA when fracture in fact hasn't happened. This kind of missing mechanism is called missing by design, since the missingness relies on answer in another variable. In this variable we willjust add a third category for poeple who did't have fracture. This mechanism of filling in is called "imputation based on logical rules", since missing-data mechanism is known.


Since we will be applying random forest model to the data, one idea is that if we have variables like age.first.hip, which is a binary variable indicating whether first time when fracture of hip happened (<50, >50).
If we introduce another category into this variable - NA, which will indicate that there was no fracture, then this variable gives exact same information as the variable break.hip, which inicated whether hip was broken but is more granular on the "yes" category.

This simple and often useful approach to imputation by adding an extra category for the variable indicating missingness is often used for unordered categorical predictors. 

```{r}

levels(dd$age.first.hip) #Adding level to a factor
dd$age.first.hip<-factor(dd$age.first.hip, levels= c(levels(dd$age.first.hip),"0"))
dd$age.first.hip[data$broken.hip==2]<-0

dd$age.first.wrist<-factor(dd$age.first.wrist,levels= c(levels(dd$age.first.wrist),"0"))
dd$age.first.wrist[data$broken.wrist==2]<-0

dd$age.first.spine<-factor(dd$age.first.spine,levels= c(levels(dd$age.first.spine),"0"))
dd$age.first.spine[data$broken.spine==2]<-0

dd.full$age.first.hip<-dd$age.first.hip
dd.full$age.first.wrist<-dd$age.first.wrist
dd.full$age.first.spine<-dd$age.first.spine
```

We will add these variables to the full data, because the information was not missing but misrepresented.

```{r}
ddm.90<-dd[which(colSums(is.na(dd)) >5935*0.9)] 
dim(ddm.90)
aggr(ddm.90, oma = c(8,1,0,0)) 

```


Other variables that are conditional on values of observed variables:

1. age.diab, time.diab (diab)
2. age.arth, type.arth (arth)
3. age.heart          (heart)
4. age.liver, still.liver(liver)


we could potentially descretize these values and create a category "not applicable" for those who do not have the disesase. For now I would like just to see whether simply having one of those condition will affect the model so I will put the variables that detail the disease aside. 

We will put these variables in the dd.imputed data set. We will try these variables in model building but take note that these varaibles are not as reliable and will try to avoid including them in the model if the model performance is not improved drastically.

```{r}

dd.imputed<-select(dd, c("age.diab", "age.liver", "age.heart", "time.diab", "type.arth","still.liver", "age.arth"))

```


```{r}
dd<-select(dd,-c(names(dd.imputed)))
dd<-select(dd,-c(names(dd.full)))

ddm.90<-dd[which(colSums(is.na(dd)) >5935*0.9)]
aggr(ddm.90, oma = c(8,2,0,0)) 

```

Moving on. Looking at the graph we see that variable test.ostep has over 93% missing and it doesn't seem like we can get much information as to why from another variable. Also since the most common tests for osteoporosis includes Bone Density Mass and we have those variables available it is safe to remove this variable from the analysis.

Other troubling variables are OSQ140Q OSQ140U, these variables has over 95% of missing values pertain to the use and frequency of use of corticosteriods. Since the variables are continuous and we have NA values for people who haven't used steroids I think it is appropriate to put value of 0, for people who haven't used any steriods.

```{r}

dd$test.ostep<-NULL
dd$OSQ140Q[data$steroid==2]<-0
levels(dd$OSQ140U)<-c(levels(dd$OSQ140U),"0")
levels(dd$OSQ140U)
dd$OSQ140U[data$steroid == 2]<-"0"

dd.full$OSQ140Q<-dd$OSQ140Q
dd.full$OSQ140U<-dd$OSQ140U

dd$OSQ140Q<-NULL
dd$OSQ140U<-NULL

```

#### 2.4.2 More than 20% missing

So we are done with these varibles and have eliminated their missing values, lets move on to the rest of the variables that had a smaller percentage of missing values. We will put the threshhold at -more than 30% missing.

```{r}

ddm.20<-dd[which(colSums(is.na(dd)) >5935*0.2)]
aggr(ddm.20, oma = c(8,1,0,0)) #oma: vector of the form c(bot,left,top,right) giving the size of the outer margins in lines of text


```

From the above plot we see that there are several patterns of missingness.
There are groups of variable on the same subject that are missing.

1. Group 1: Smoking habit variable
2. Group 2: Alcohol consumption variables
3. Group 3: Weight related variables
4. Group 4: Bone mass density (BMD) variables 

##### Group 1: Smoking habit variable

From online search I have identified that variale SMQ020 and SMQ040 pertains to smaking habits. 
SMQ020 - do you smoke? 1-yes, 2-no
SMQ040 - how often? 1- daily, 2-sometimes, 3-not at all.

when looking at the plot it seems that people who responded that they do not smoke did not indicate anything in SMQ040 variable. This means that we can put in the value of 3 in SMQ040 whenever SMQ020 is 2, which is similar to what we did previously.


```{r}
dd$SMQ040[data$SMQ020==2]<-3
```


```{r}
ddm.20<-dd[which(colSums(is.na(dd)) >5935*0.2)]
aggr(ddm.20, oma = c(8,2,0,0)) #oma: vector of the form c(bot,left,top,right) giving the size of the outer margins in lines of text


```

##### Group 2: Alcohol consumption variables

Next up are alcohol consumption variables. These variables are highly unreliable because they involve sensitive information. However we could fill in some NaN values. We have 3 variables, average day consumption (av.alc), how many alcoholic beveragesin a year (alc12), and number of days you consumed more than 5 drinks in a day (5drink.days). We will first fill in values of "0" in av.alc and 5drink.days when person has consumed less that 12 drinks last year.


```{r}

dd$`5drinks.days`[dd$`12alc`==2]<-0
dd$av.alc[dd$`12alc`==2]<-0
ddm.20<-dd[which(colSums(is.na(dd)) >5935*0.2)]
aggr(ddm.20, oma = c(8,2,0,0))

```
This has reduced number of missing values but has not eliminated them.
We will put that variable in the dd.imputed dataset.

```{r}
dd.imputed$`5drink.days`<-dd$`5drinks.days`
dd.imputed$av.alc<-dd$av.alc

dd$`5drinks.days`<-NULL
dd$av.alc<-NULL
ddm.20<-dd[which(colSums(is.na(dd)) >5935*0.2)]
aggr(ddm.20, oma = c(8,2,0,0))

```

##### Group 3: Weight related variables

There is only 1 weight related varaible with missing values, self reported wight 10 years ago. This varaible can be safely imputed because we have other weight variables such as BMI, self weight, 2 year sample weight taken during medical examination and others. We will put this variable in dd.impute dataset.

```{r}

dd.imputed$weight10.self<-dd$weight10.self
dd$weight10.self<-NULL

```

##### Group 4: Bone mass density (BMD) variables.

The last but the the most important ones are the bone density mass (BMD) variables. The variables are measurements of BMD on femur, femur.neck, trochanter, intertrochanter and ward's triangle which are all located in pelvis. It seems that these measurements were done only when certain criteria was satisfied. This pattern of missingness is called univeriate non-response. 

However this missingness is not the same we have encountered before, because it is not missingng explicitly by survey-design (Smoke? -yes, -no. How many packs a day -numeric)
It does seem like some criteria needs to be satiffied befor the tests are done but we don't know which.

# Think what you can do with these?

I am still thinking about what would be the best way to impute these values and will comeback to this later. For now I will put all of these variables into the dd.impute.


Lets look at the remaining variables which are not in dd.full and not in dd.imputed yet, which means that they have less than 20% missing values.

```{r}

dd.imputed$id<-data$id
ddm.20$id<-data$id
dd.imputed<-merge.data.frame(dd.imputed, ddm.20, by = "id" )
ddm.20$id<-NULL
ddm.20$weight10.self<-NULL

dd<-select(dd,-c(names(ddm.20)))
aggr(dd)

```


Since the rest of the variables have less than 0.1% missing values we will simply eliminate these observations. I will impute the values for variables that have to do with weight and hight (WHD010) and put them in  dd.full, because we have other very similar variables to impute these variables from. Missing observations from smoking behaviour variables will be eliminated and the alcohol variable put in the dd.impute.

```{r}

dd.imputed$`12alc`<-dd$`12alc`
dd$`12alc`<-NULL
dd.full$id<-data$id
dd$id<-data$id
dd.full<-full_join(dd.full,dd, by = "id")
dd.full$id<-NULL
```


### Imputation using missForest.

Now we will continue working on dd.full which has 26 variables. We will use package missForest to impute the missing values. It performs a non-parametric value imputation using Random forest. 

"missForest is used to impute missing values particularly in the case of mixed-type data. It can be used to impute continuous and/or categorical data including complex interactions and nonlinear relations. It yields an out-of-bag (OOB) imputation error estimate. Moreover, it can be run parallel to save computation time"

```{r}
#install.packages("missForest")
library(missForest)

system.time(dd.forest<-missForest(dd.full)) 
dd.forest.full<-dd.forest$ximp

dd.forest.full$milk30.day<-dd.forest.full$`30day.milk`
dd.forest.full$`30day.milk`<-NULL


dd.forest.full$fracture<-as.factor(dd.forest.full$fracture) 

```

## Modelling.

Finally we can start playing around with modelling. We will be using Random Forest model to predict the risk of fracture.
Random Forest is a very powerful ensembling machine learning algorithm which works by creating multiple decision trees and then combining the output generated by each of the decision trees. Decision tree is a classification model which works on the concept of information gain at every node. For all the data points, decision tree will try to classify data points at each of the nodes and check for information gain at each node. It will then classify at the node where information gain is maximum. It will follow this process subsequently until all the nodes are exhausted or there is no further information gain.


First, lets split our data set into a training and testing data sets

```{r}
set.seed(100)
train <- sample(nrow(dd.forest.full), 0.7*nrow(dd.forest.full), replace = FALSE)

dd.train<-dd.forest.full[train,]
dd.test<-dd.forest.full[-train,]


xqplot(dd.train)

```


```{r}

xqplot(dd.test)

```


First we will run model with default parameters and then we can fine-tune these parameters later. Two basin parameters are mtry - number of variables randomly sampled at each step (sqrt(p) for classification, (p/3) for regression), and ntree - number of individual trees grown.


```{r}

system.time(model1<-randomForest(fracture ~ ., data = dd.train, importance = TRUE, ntree =500, mtry = 5))

model1

test.pred<-predict(model1,dd.test)
train.pred<-predict(model1,dd.train)
table(train.pred, dd.train$fracture)
table(test.pred, dd.test$fracture)
acc<-mean(test.pred==dd.test$fracture)
acc

```


Surprisingly the accuracy of the model is 99.9% which quite unsettling.
Usually with unbalanced data this can happen if you just classify everything 

lets look at the variables are giving us such good prediction for the model.

```{r}

model1$importance %>% round()


```



It seems that the categorical variable with when fracture occured gives a very good prediction power, however if we want to predict fracture based on age we have to have the variable with ages when fracture occured vs regular ages. Otherwise this categorical variable basically contains the same info as the fracture variable. We will dichotomize the variable age into  <50 and >50 and merge it with these variables.

```{r}

dd.forest.full$age.first.hip[dd.forest.full$age<50]<-1
dd.forest.full$age.first.hip[dd.forest.full$age>=50]<-2

dd.forest.full$age.first.wrist[dd.forest.full$age<50]<-1
dd.forest.full$age.first.wrist[dd.forest.full$age>=50]<-2

dd.forest.full$age.first.wrist[dd.forest.full$age<50]<-1
dd.forest.full$age.first.wrist[dd.forest.full$age>=50]<-2


```

These variable will help us see if age less than 50 or age more than 50 can help us predict the risk of osteoporosis. Lets update the training and testing sets.


```{r}

set.seed(100)
train <- sample(nrow(dd.forest.full), 0.7*nrow(dd.forest.full), replace = FALSE)

dd.train<-dd.forest.full[train,]
dd.test<-dd.forest.full[-train,]

system.time(model2<-randomForest(fracture ~ ., data = dd.train, importance = TRUE, ntree =500, mtry = 5))

model2

```

Lets look at the confusion matrix and accuracy.

```{r}

test.pred<-predict(model2,dd.test)
train.pred<-predict(model2,dd.train)

table(train.pred, dd.train$fracture)

```


```{r}

table(test.pred, dd.test$fracture)

```

```{r}

acc<-mean(test.pred==dd.test$fracture)
acc

```














Measurement of L1 - L4 are measurements of BMD associate with L1- L4 vertibrae.
spine BMD? is it total or avetage BMD? how they measure.


```{r include = FALSE}

#dd.core$av.alc[dd.core$`12alc`==2]<-0
#dd.core$`5drinks.days`[dd.core$`12alc`==2]<-0

#ddm.100<-dd.core[which(colSums(dd.core=="NaN" | dd.core=="NA") >100)]
#aggr(ddm.100, oma = c(8,2,0,0))
#To do next:

#1.BMI and weight self past 10 years, look for correlation and maybe we can impute

#2. Who did not want to answer questions about alcohol consumption?

#3. Put a model on all the bone stuff and then impute. Why did these people not get tested?

#4. for the variables: femur.tot, femur.neck,trochanter, intertrochanter and wards we see the pattern of missingness which is calles Little and Ruben, multivariate two pattern. In this case since the missing values are on the same individuals it seems reasonable to say that there is some criteria which needs to satisfied to get bone density testing on these particular bones. I will check the pattern of missingness angainst some of the variables as (age, whether bone was broken) using logistic regression to see whether this is true

#5. One thing to keep in mind is the order in which variables were aquired. For example there could be strong relationship between fracture and test for BMD but if the test was conducted after the fracture happened then we can not use it to predict fracture. But if test is done before the fracture appears, based on some criteria like age then we can use that info to predict the chance of fracture in that individual based on the test.





```

# Num.broken variables for more refined analysis.

By looking at the first three variables (num.broken.hip, num.broken.spine, num.broken.wrist) I notice that these variables have NAN when fracture in fact hasn't happened. I will assign value of 0 to (num.broken.hip, num.broken.spine, num.broken.wrist) when (broken.hip, broken.spine,broken.wrist) is 2, meaning no fracture happened

I notice that these variables have NAN when fracture in fact hasn't happened. 
This kind of missing mechanis is called Missing at Random (MAR), since the missingness relies on observed values of another variable.


```{r}

levels(dd$num.broken.hip) #Adding level to a factor
dd$num.broken.hip<-factor(dd$num.broken.hip, levels= c(levels(dd$num.broken.hip),"0"))
dd$num.broken.hip[dd$broken.hip==2]<- 0


dd$num.broken.wrist<-factor(dd$num.broken.wrist,levels= c(levels(dd$num.broken.wrist),"0"))
dd$num.broken.wrist[dd$broken.wrist==2]<-0

dd$num.broken.spine<-factor(dd$num.broken.spine,levels= c(levels(dd$num.broken.spine),"0"))
dd$num.broken.spine[dd$broken.spine==2]<-0

```


These variables could help us see whether age has anything to do with probability of fracture as it tells us at what age fracture occured.
One solution could be creating a variable "Relevant age" which will have current age for people who did't have the fracture and age of fracture which had occured.

